{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_datareader import data, wb\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#cor plot index use is inconsistant with some other function maybe HRP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run /Users/davidfitzpatrick/Desktop/Codes/Python_codes/Fun_codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run /Users/davidfitzpatrick/Desktop/Codes/Python_codes/Finance_codes/tools/helpers.ipynb\n",
    "\n",
    "#read in stock data\n",
    "#tickers = ['SPY', 'SPYC']\n",
    "#start = datetime(2020,1,1)\n",
    "#end = datetime(2021,7,20)\n",
    "#returns = import_data(tickers,start,end)\n",
    "\n",
    "\n",
    "#TIP: use join when have dataframes of different lengths and just want to keep the intersection of rows\n",
    "\n",
    "\n",
    "# plot dividends\n",
    "    # pd.DataFrame(port[\"IVV_Div\"]/port[\"IVV\"]).resample('AS').sum().plot()\n",
    "\n",
    "# annualize returns\n",
    "    # returns.resample(\"A\").apply(lambda x: ((x + 1).cumprod()-1).last(\"D\"))\n",
    "\n",
    "#for company info\n",
    "    #si.get_quote_table(\"SPY\", dict_result = False)\n",
    "\n",
    "# get weekday\n",
    "    # week['weekday'] = week['Date'].apply(lambda x: x.weekday())\n",
    "    \n",
    "#bid ask info \n",
    "    #si.get_quote_table(\"SPY\", dict_result = False)\n",
    "\n",
    "#no_dupes = [x for n, x in enumerate(a) if x not in a[:n]] #print no_dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned up, all functions are intended to run on dataframes of returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EX\n",
    "\n",
    "#This model assumes the risk free rate is constant over the period\n",
    "\n",
    "#t = 1 #number to makes std and means equivelent to yearly returns so 12 for monthly, 52 for weekly and 252 for daily\n",
    "#posLB = 0   #lower bound on position exposure\n",
    "#posUB = 1  #upper bound on position exposure\n",
    "#tickers = col\n",
    "#increments_for_graph = .001\n",
    "\n",
    "#make sure returns are decimal\n",
    "#Cov = np.array((returns[tickers].loc[start_year:end_year]/100).cov())\n",
    "#muf = returns['Cash'].loc[start_year:end_year].mean() /100\n",
    "#mean_vect = returns[col].loc[start_year:end_year].mean() /100 #this data gives percents as whole numbers\n",
    "\n",
    "\n",
    "#depends on scipy.optimize.linprog and qpsolvers.solve_qp\n",
    "def mean_var_opt(posLB, posUB, t, tickers, increments_for_graph, Cov, muf, mean_vect):\n",
    "    \"\"\"\n",
    "    posLB,posUB: upper and lower bounds for individual positions\n",
    "    t: number to multiply expected returns by to make them yearly\n",
    "    tickers: list of column names\n",
    "    increments_for_graph: decreasing this number increases the number of portfolios calculated\n",
    "    Cov: covariance of returns\n",
    "    muf: should be a single value\n",
    "    mean_vect: should be mean of dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    Cov = 2 * np.array(Cov)\n",
    "    mean_vect = mean_vect * t\n",
    "    muf  = muf * t\n",
    "    m = len(tickers)\n",
    "\n",
    "    from scipy.optimize import linprog\n",
    "    #set equality constraints. I want my positions to sum to 1\n",
    "    A_eq = np.array((1,)*m).reshape(1,-1)\n",
    "    b_eq = [1]\n",
    "\n",
    "    # set bounds on leverage. We can short but we can not go long\n",
    "    bounds = np.vstack([(posLB,)*m, (posUB,)*m]).T\n",
    "\n",
    "    #find min and max return\n",
    "    min_rtn = mean_vect @ linprog(c=mean_vect, A_eq = A_eq, b_eq = b_eq, bounds=bounds).x\n",
    "    max_rtn = mean_vect @ linprog(c=-mean_vect, A_eq = A_eq, b_eq = b_eq, bounds=bounds).x\n",
    "\n",
    "    from qpsolvers import solve_qp\n",
    "    muP = np.arange(min_rtn * .995,max_rtn * .995, increments_for_graph)\n",
    "    sdP = np.zeros(len(muP))\n",
    "    weights = np.zeros([len(muP),m])\n",
    "\n",
    "    q_vec = np.zeros(m).reshape(-1,)\n",
    "    G = np.zeros([m,m])\n",
    "    h = np.zeros(m)\n",
    "    A = np.vstack([np.array((1,)*m).reshape(1,-1),mean_vect])\n",
    "\n",
    "    #calculate each optimized portfolio for each mean\n",
    "    for i in range(len(muP)):\n",
    "        b = np.array([1,muP[i]])\n",
    "        weights[i] = solve_qp(P=Cov, q = q_vec, G=G, h=h, A=A, b=b, lb = bounds[:,0], ub = bounds[:,1])\n",
    "        sdP[i] = np.sqrt(weights[i] @ (Cov/2) @ weights[i]) * np.sqrt(t)\n",
    "    \n",
    "    portfolios = pd.DataFrame({'Returns': muP, 'Volatility': sdP,})\n",
    "    for counter, symbol in enumerate(tickers):\n",
    "        portfolios[symbol+' weight'] = [w[counter] for w in weights]\n",
    "    \n",
    "    portfolios['Sharpe'] = (portfolios['Returns']-muf) / portfolios['Volatility'] #note using monthly returns and vol\n",
    "\n",
    "    tangent = portfolios.iloc[[portfolios.Sharpe.argmax()]]\n",
    "    min_var = portfolios.iloc[[portfolios.Volatility.argmin()]]\n",
    "    efficient_portfolios = portfolios[portfolios['Returns'] >= min_var.Returns.values[0]-.02] #the .02 is arbitrary to see a bit below the min var port too\n",
    "\n",
    "    return tangent, min_var, efficient_portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning give 1d numpy arrays ex.\n",
    "#x1 = tested_params_rand[:,0]\n",
    "#x2 = tested_params_rand[:,1]\n",
    "#y = np.ravel(sharpe_rand)\n",
    "def contourplot(x1,x2,y,lvlnum):\n",
    "    plt.figure(figsize=(18,6))\n",
    "    plt.tricontourf(x1, x2, y, levels=lvlnum, cmap=\"seismic\")\n",
    "    plt.scatter(x1, x2, alpha = .9, c = y, cmap = 'seismic')\n",
    "    cbar = plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note: bayes opt does better if function being evaluated is in the range of 1-1000\n",
    "#note: if mean error is high increase initial sample size\n",
    "\n",
    "#bayes opt\n",
    "#from numpy.random import normal, uniform, random\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from numpy.random import uniform\n",
    "from scipy.stats import norm\n",
    "import time\n",
    "\n",
    "\n",
    "#from sklearn.svm import SVC\n",
    "#def func_approx(x):\n",
    "#    g = x[0]\n",
    "#    w = x[1]\n",
    "#    real_model = SVC(gamma = g/10, class_weight={0: w, 1: 1}) #'balanced'\n",
    "#    real_model.fit(xTr, np.ravel(yTr))\n",
    "#    predictions = real_model.predict(xVal)\n",
    "#    risk_layer_performance = np.multiply(y[train_val_threshold:val_test_threshold],predictions)\n",
    "#    series = pd.Series(risk_layer_performance)\n",
    "#    geo_rtn = geometric_return(series).values[0]\n",
    "#    std = series.std()    \n",
    "#    return geo_rtn/(std+.000001)\n",
    "\n",
    "#sample of features(N) which sum to 1. Sample M times\n",
    "#def constrained_samp(M):\n",
    "#    U=np.zeros([M,2])\n",
    "#    U[:,0] = uniform(1,1000,M)\n",
    "#    U[:,1] = uniform(1,50,M)\n",
    "#    return(U.astype(int))\n",
    "\n",
    "\n",
    "\n",
    "#sample of features(N) which sum to 1. Sample M times\n",
    "from numpy.random import uniform\n",
    "#def constrained_samp(M,N):\n",
    "#    H=np.zeros([M,N+1])\n",
    "#    U=np.zeros([M,N])\n",
    "#    for j in range (0,M):\n",
    "#        for i in range(1,N): \n",
    "#            H[j,i] = round(uniform(0,1),2)\n",
    "#        H[j,N] = 1\n",
    "#        H.sort()\n",
    "#        for i in range(1,N+1):\n",
    "#            U[j,i-1] = H[j,i] - H[j,i-1]\n",
    "#    return(U)\n",
    "\n",
    "# probability of improvement acquisition function\n",
    "def acquisition(X, Xsamples, model):\n",
    "    # calculate the best surrogate score found so far\n",
    "    yhat = model.predict(X)\n",
    "    best = max(yhat)\n",
    "    # calculate mean and stdev via surrogate function\n",
    "    mu, std = model.predict(Xsamples, return_std=True)\n",
    "    mu = mu.reshape(-1,)\n",
    "    std = std.reshape(-1,)\n",
    "    # calculate the probability of improvement\n",
    "    probs = norm.cdf((mu - best) / (std+1E-9))\n",
    "    return probs\n",
    "\n",
    "# optimize the acquisition function\n",
    "def opt_acquisition(X, y, model,search_samp):\n",
    "    # random search, generate random samples\n",
    "    Xsamples = constrained_samp(search_samp)\n",
    "    # calculate the acquisition function for each sample\n",
    "    scores = acquisition(X, Xsamples, model)\n",
    "    # locate the index of the largest scores\n",
    "    ix = np.argmax(scores)\n",
    "    return Xsamples[ix]\n",
    "\n",
    "def bayes_opt(starting_data = 50, search_samp = 100, optimization_steps = 50):\n",
    "    \"\"\"\n",
    "    startig data: number of actual points to start with\n",
    "    search samp: number of points to sample at each optimization step\n",
    "    optimization_steps: number of updates\n",
    "    \"\"\"\n",
    "    \n",
    "    t0 = time.time()\n",
    "\n",
    "    #samples\n",
    "    X = constrained_samp(starting_data)\n",
    "    ys = np.asarray([func_approx(x) for x in X]).reshape(-1,1) ####\n",
    "    # define the model\n",
    "    model = GaussianProcessRegressor()\n",
    "    model.fit(X, ys)\n",
    "\n",
    "    # perform the optimization process\n",
    "    error=[]\n",
    "    for i in range(optimization_steps):\n",
    "        # select the next point to sample\n",
    "        point = opt_acquisition(X, ys, model, search_samp)  #####\n",
    "        # sample the point\n",
    "        actual = func_approx(point) ####\n",
    "        # summarize the finding\n",
    "        est = model.predict(point.reshape(1,-1))\n",
    "        wrongness=abs(actual-est)/(est+.0001)\n",
    "        error.append(wrongness.item())\n",
    "        # add the data to the dataset\n",
    "        X = np.vstack((X, point))\n",
    "        ys = np.vstack((ys, [[actual]]))\n",
    "        # update the model\n",
    "        model.fit(X, ys)\n",
    "    \n",
    "    # best result\n",
    "    X = np.round(X,2)\n",
    "    ix = np.argmax(ys)\n",
    "    print('Best Result: \\nWeights=%s, \\n function max=%.6f' % (tuple(X[ix]), ys[ix]))\n",
    "    print('\\nMean Error=%.3f' % (np.mean(error)))\n",
    "    t1 = time.time()\n",
    "    print(\"time\",t1-t0)\n",
    "    return(X,ys,ix,error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note intervals might behave oddly if using an extremely volitile asset\n",
    "#returns dataframe, array and plot\n",
    "\n",
    "def conditional_performance(df,independant_var, intervals=[.02,.01,.005,0,-.005,-.01,-.02], relative_performance=False, plot = True):\n",
    "    select_intervals = intervals.copy()\n",
    "    select_intervals.insert(0,1)\n",
    "    select_intervals.append(-1)\n",
    "    \n",
    "    ave_rtn_in_interval = []\n",
    "    \n",
    "    outperformance, probabilities = np.zeros([len(select_intervals)-1,len(df.columns)-1]), np.zeros([len(select_intervals)-1])\n",
    "    \n",
    "    for i in range(1,len(select_intervals)):\n",
    "        df_conditioned = df[(df[independant_var]<select_intervals[i-1]) & (df[independant_var]>select_intervals[i])]\n",
    "        ave_rtn_in_interval.append(df_conditioned[independant_var].mean())\n",
    "        probabilities[i-1] = len(df[(df[independant_var]<select_intervals[i-1]) & (df[independant_var]>select_intervals[i])])/len(df)\n",
    "        if relative_performance == True:\n",
    "            outperformance[i-1] = np.array( df_conditioned.sub(df_conditioned[independant_var],axis=0).drop([independant_var],axis=1).mean() )\n",
    "        else:\n",
    "            outperformance[i-1] = np.array( df_conditioned.drop([independant_var],axis=1).mean() )\n",
    "            \n",
    "    \n",
    "    cond_rtns = pd.DataFrame(outperformance, index = ave_rtn_in_interval, columns = returns.columns.drop(independant_var))\n",
    "    \n",
    "    if plot == True:\n",
    "        for i in range(len(cond_rtns.columns)):\n",
    "            plt.scatter(cond_rtns.index,cond_rtns[cond_rtns.columns[i]], label=\"{} data\".format(cond_rtns.columns[i]))\n",
    "        for i in intervals:\n",
    "            plt.axvline(x=i)\n",
    "        plt.legend(bbox_to_anchor=(1, 1))\n",
    "        plt.xlabel(independant_var+' daily return')\n",
    "        plt.ylabel('daily return')\n",
    "        plt.title(\"asset returns conditional on \"+independant_var+\" return\")\n",
    "\n",
    "    return cond_rtns, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosscorr(leader, follower, horiz_graph_bounds, plot=False, title=\"Title\"): \n",
    "\n",
    "    x = np.arange(len(leader))\n",
    "    y1 = follower\n",
    "    y2 = leader\n",
    "    npts = len(y1)\n",
    "\n",
    "    lags = np.arange(-npts + 1, npts)\n",
    "    ccov = np.correlate(y1 - y1.mean(), y2 - y2.mean(), mode='full')\n",
    "    ccor = ccov / (npts * y1.std() * y2.std())\n",
    "\n",
    "    #fig, axs = plt.subplots(nrows=2)    \n",
    "    #fig.subplots_adjust(hspace=0.4)   \n",
    "    #ax = axs[0]   \n",
    "    #ax.plot(x, y1, 'b', label='Follower')\n",
    "    #ax.plot(x, y2, 'r', label='Leader')    \n",
    "    #ax.set_ylim(-.1, 1)    \n",
    "    #ax.legend(loc='upper right', fontsize='small', ncol=2)\n",
    "\n",
    "    if(plot==True):\n",
    "        fig, axs = plt.subplots(nrows=1)\n",
    "        ax = axs#[1]\n",
    "        ax.plot(lags[horiz_graph_bounds:-horiz_graph_bounds], ccor[horiz_graph_bounds:-horiz_graph_bounds]) #npts-1, npts-1\n",
    "        ax.set_ylim(-.7, .9)\n",
    "        ax.set_ylabel('Correlation')\n",
    "        ax.set_xlabel('Days between Leader and Follower')\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    maxlag = np.argmax(ccor[:])#npts-1\n",
    "    maxcorr = np.max(ccor[:])#npts-1\n",
    "    print(title,\": Max correlation is at lag\", maxlag-npts+1,\"\\nCorrelation is\", round(maxcorr,2)) \n",
    "    # from following https://currents.soest.hawaii.edu/ocn_data_analysis/_static/SEM_EDOF.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#growth of 1 dollar invested\n",
    "def dollar_growth(returns):\n",
    "    return ((returns+1).cumprod())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate geometric return from imported return data\n",
    "def geometric_return(data):\n",
    "    return ((data+1).cumprod()[-1:].abs()**(1/(len(data)))-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(tickers, start, end, give = \"R\", adj_close = True, dividend = True):#read in stock data  \n",
    "    if give=='D':\n",
    "        adj_close=False\n",
    "        dividend = True\n",
    "    \n",
    "    stocks = pd.DataFrame()\n",
    "    if (adj_close == True):\n",
    "        dividend = False\n",
    "        stocks[tickers] = data.DataReader(tickers, 'yahoo', start, end)['Adj Close']\n",
    "    else:\n",
    "        stocks[tickers] = data.DataReader(tickers, 'yahoo', start, end)['Close']\n",
    "            \n",
    "        if (dividend == True): \n",
    "            stock_div = pd.DataFrame(index=stocks.index)\n",
    "            for tick in tickers:\n",
    "                stock_div[tick+' div'] = data.DataReader(tick, 'yahoo-dividends', start, end)[\"value\"]\n",
    "      \n",
    "    just_stocks=stocks\n",
    "    if (dividend == True):\n",
    "        stocks = pd.concat([stocks,stock_div],axis=1)\n",
    "        stocks = stocks.fillna(0)\n",
    "        \n",
    "    if(give == \"R\"):\n",
    "        #Adding dividends to price appreciation on a daily percent return basis -useful for all analysis\n",
    "        raw_returns=pd.DataFrame()\n",
    "        for tick in tickers:\n",
    "            if (dividend == True):\n",
    "                raw_returns[tick] = stocks[tick].pct_change() + stocks[tick+' div']/stocks[tick] \n",
    "                #dividends are actually added on the day before they should be added but this error is neglegible\n",
    "            else:\n",
    "                raw_returns[tick] = stocks[tick].pct_change()\n",
    "        returns=raw_returns[1:]\n",
    "        return returns[np.isfinite(returns).all(1)]\n",
    "        \n",
    "    if(give == \"P\"):\n",
    "        return just_stocks\n",
    "    \n",
    "    if(give==\"D\"):\n",
    "        dividend = pd.DataFrame(np.array(stocks.iloc[:,len(just_stocks.columns):])/np.array(stocks.iloc[:,:len(just_stocks.columns)])).set_index(just_stocks.index)\n",
    "        dividend.columns=just_stocks.columns\n",
    "        return dividend\n",
    "    \n",
    "    #importing financial data from yahoo finance\n",
    "#commodity_futures = ['GC=F', 'SI=F', 'CL=F']\n",
    "#cryptocurrencies = ['BTC-USD', 'ETH-USD', 'XRP-USD']\n",
    "#currencies = ['EURUSD=X', 'JPY=X', 'GBPUSD=X']\n",
    "#mutual_funds = ['PRLAX', 'QASGX', 'HISFX']\n",
    "#us_treasuries = ['^TNX', '^IRX', '^TYX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_curve(start,end,plot=True):\n",
    "    \n",
    "    if (plot==True):\n",
    "        Interest_rates = ['^IRX','^TNX'] # '^FVX', '^TYX'\n",
    "        IR = import_data(Interest_rates,start,end, give =\"P\", dividend=True)\n",
    "        IR[\"yd_cve\"]=IR[\"^TNX\"]-IR[\"^IRX\"]\n",
    "        IR[\"yd_cve\"].plot()\n",
    "        plt.title(\"10 year - 3 month rate\")\n",
    "        plt.show(block=True)\n",
    "    if (plot==False):\n",
    "        Interest_rates = ['^IRX','^FVX','^TNX','^TYX']\n",
    "        IR = import_data(Interest_rates,start,end, give =\"P\", dividend=True)\n",
    "        return IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core.common import flatten\n",
    "#from diversified ports\n",
    "#backtest shell function\n",
    "def backtest(weights, col, dataf, days_between_rebalance, rebalance_func, show_weights=False, wealth = 1, RebalanceOffset=0, give='R',formation = 0): #cols allows users to backtest subsets of dataframe\n",
    "    \"\"\"\n",
    "    weights: tells the function the number of assets. sets the starting weights and is the weights for constant rebalancing\n",
    "    wealth: sets starting value\n",
    "    RebalanceOffset: to offset day of rebalances by these values to avoid rebalance timing luck\n",
    "    give: return returns or prices\n",
    "    formation: number of days covariance matrix is formed over\n",
    "    \n",
    "    \"\"\"\n",
    "    colin=[0]*len(weights)\n",
    "    port=[0]*len(weights)\n",
    "    worth=[wealth]\n",
    "\n",
    "    for n in range(len(weights)):\n",
    "        colin[n] = dataf.columns.get_loc(col[n])\n",
    "        port[n]=wealth*weights[n]\n",
    "\n",
    "    for n in range (formation,len(dataf)):\n",
    "        port = np.multiply(port, np.array((1+dataf.iloc[n,colin])))\n",
    "\n",
    "        end_of_day = sum(port)\n",
    "        worth.append(end_of_day)\n",
    "        \n",
    "        if ((n + RebalanceOffset)% days_between_rebalance==0):        #rebalance\n",
    "            port=rebalance_func(dataf, n, formation, port, weights, sum(port))\n",
    "            if show_weights == True:\n",
    "                print(np.round(np.array(port/sum(port)),2))\n",
    "            \n",
    "                \n",
    "    if (give=='R'):            \n",
    "        p_rtn = pd.DataFrame(worth).pct_change()[1:].set_index(dataf.index[formation:])\n",
    "        return(p_rtn)\n",
    "    if (give=='P'):\n",
    "        return(pd.DataFrame(worth[1:]).set_index(dataf.index[formation:]))\n",
    "    \n",
    "def constant_weight_rebalance(stocks, n, formation_period, port, weights, total):  \n",
    "   #intended for monthly rebalancing\n",
    "    update = np.subtract(np.array(weights), np.array(port/total))/4\n",
    "    new_port = np.add(np.array(port/total),update)    #update portfolio weights\n",
    "    #print(np.round(new_port,2),n)                       #print weights\n",
    "    return total * np.array(weights)\n",
    "\n",
    "def ivp_rebalance(stocks, n, formation_period, port, weights, total):\n",
    "    cov = stocks[n-formation_period : n - 1].cov() \n",
    "    ivp = list(flatten(getIVP(cov).reshape(-1,1)))\n",
    "    return total * np.array(ivp)\n",
    "\n",
    "def hrp_rebalance(stocks, n, formation_period, port, weights, total):\n",
    "    formation = stocks[n - formation_period : n - 1]\n",
    "    cov,corr = formation.cov(),formation.corr()     #2) compute correl matrix           \n",
    "    \n",
    "    dist=correlDist(corr)                #3) cluster\n",
    "    link=sch.linkage(dist,'single')\n",
    "    sortIx=getQuasiDiag(link)\n",
    "    sortIx=corr.index[sortIx].tolist()   # recover labels\n",
    "    hrp=getRecBipart(cov,sortIx)         #allocation  \n",
    "    hrp = np.array(hrp[stocks.columns])  #sort\n",
    "    return total * hrp\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import squareform\n",
    "def hrp_rebalance(stocks, n, formation_period, port, weights, total):\n",
    "    #formation period is 252 * 2\n",
    "    #hrp recursively bisect and equalize risk budgets at every level of heirarchy\n",
    "    formation = stocks[n - formation_period : n - 1]\n",
    "    cov,corr = formation.cov(),formation.corr()     #2) compute correl matrix       \n",
    "    dist=correlDist(corr)                #3) cluster\n",
    "    link=sch.linkage(squareform(dist),'single')   \n",
    "    sortIx=getQuasiDiag(link)\n",
    "    sortIx=corr.index[sortIx].tolist()   # recover labels\n",
    "    hrp=getRecBipart(cov,sortIx,formation.mean()) # allocation \n",
    "\n",
    "    #for rebalancing every month (21 business days), uses sorted HRP\n",
    "    update = np.subtract(np.array(hrp[stocks.columns]), np.array(port/total))/4   #reduce turnover and rebalance timing luck by only moving towards optimal value\n",
    "    \n",
    "    #print(abs(update).sum()/2)                       #print turnover\n",
    "    new_port = np.add(np.array(port/total),update)    #update portfolio weights\n",
    "    #print(np.round(new_port,2),n)                       #print weights\n",
    "    return total * new_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "def windowed_view(x, window_size):\n",
    "    \"\"\"Creat a 2d windowed view of a 1d array.\n",
    "\n",
    "    `x` must be a 1d numpy array.\n",
    "\n",
    "    `numpy.lib.stride_tricks.as_strided` is used to create the view.\n",
    "    The data is not copied.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> x = np.array([1, 2, 3, 4, 5, 6])\n",
    "    >>> windowed_view(x, 3)\n",
    "    array([[1, 2, 3],\n",
    "           [2, 3, 4],\n",
    "           [3, 4, 5],\n",
    "           [4, 5, 6]])\n",
    "    \"\"\"\n",
    "    y = as_strided(x, shape=(x.size - window_size + 1, window_size),\n",
    "                   strides=(x.strides[0], x.strides[0]))\n",
    "    return y\n",
    "\n",
    "def max_dd(prices):\n",
    "    cum_returns = (1 + prices).cumprod()\n",
    "    drawdown =  1 - prices.div(prices.cummax())\n",
    "    return drawdown.expanding().max().iloc[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Turbulence_indicator(returns): #reduces dataset by 15 days because covariance matrix is unstable before that\n",
    "    turbulence=[]\n",
    "    for i in range(15,len(returns)):\n",
    "        deviation = returns.iloc[i]-returns[:i].mean() #[:i] prevents information bleed\n",
    "        inverse = np.linalg.inv(returns[:i].cov())     #[:i] prevents information bleed\n",
    "        turb = np.sqrt(np.matmul(np.matmul(deviation.T,inverse),deviation))\n",
    "        turbulence.append(turb)\n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "    AllData = returns[15:]\n",
    "    AllData['Turbulence']=turbulence\n",
    "    pd.options.mode.chained_assignment = 'warn'  # default='warn'\n",
    "    return AllData\n",
    "\n",
    "#Turbulence indicator- I use the square root because the magnitude of the indicator is higher for daily data\n",
    "#starts 15 days in for stability reasons. inverse covariance matrix is large and sometimes negative for small sample sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta(regressor, data, start, end): #Monthly beta\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    #from sklearn.base import clone\n",
    "    #import sklearn\n",
    "    lm = LinearRegression()\n",
    "    riskfree = 'VFISX' #VFISX is a poor risk_free rate maturity currently being 3 years, but it's convenient\n",
    "    #returns = import_data([regressor,riskfree]+tickers,start,end, give =\"R\",dividend=True) alt method\n",
    "    returns_internal = import_data([regressor,riskfree],start,end, give =\"R\",dividend=True)\n",
    "    all_data = returns_internal.join(data,how='inner')\n",
    "    for col in (all_data.columns):\n",
    "        all_data[col] = all_data[col]-all_data[riskfree]\n",
    "    adjusted = all_data.drop(riskfree,axis=1).resample(\"M\").apply(lambda x: ((x + 1).cumprod()-1).last(\"D\"))\n",
    "    \n",
    "    coef=[]\n",
    "    for col in (adjusted.columns[1:]):\n",
    "        X = np.array(adjusted[regressor]).reshape(-1,1)\n",
    "        y = np.array(adjusted[col]).reshape(-1,1)\n",
    "        lm.fit(X,y) #use all data as training data because we arn't predicting anything\n",
    "        coef.append(lm.coef_[0])\n",
    "        \n",
    "    coeff_df = pd.DataFrame(coef,columns=[\"Beta\"]).set_index(adjusted.columns[1:]) \n",
    "    print(coeff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the monthly time series of correlation\n",
    "\n",
    "def cor_plot(start_year, end_year, dataset, SandP,lines_per_plot,num_plots,lower_y):\n",
    "    cor = pd.DataFrame()\n",
    "    month = [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"]\n",
    "    i=0\n",
    "    j=0\n",
    "    for j in np.arange(start_year,end_year+1):\n",
    "        for i in np.arange(1,13):\n",
    "            cor[month[i-1]+str(j)] = correlation(dataset,SandP,datetime(j,i,1), datetime(j,i,28))       \n",
    "    for i in np.arange(0,num_plots,1):\n",
    "        print(cor[i:i+lines_per_plot].T.rolling(4).mean().plot().set_ylim(lower_y, 1))\n",
    "        \n",
    "def correlation(dataset,SandP,start,end):\n",
    "    return dataset[start:end].corr()[SandP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMA_Backtest(SMA_len, return_len, data, initial):\n",
    "    timing = initial\n",
    "    hold = 'cash'\n",
    "    acct_value = []\n",
    "    trades=[]\n",
    "    \n",
    "    for n in range (SMA_len, len(data)-return_len-np.mod(len(data)-SMA_len, return_len),return_len): \n",
    "        #the stopping point maximises the loops us of dtaset without falling outside of it.\n",
    "        price = data.iloc[n]\n",
    "        next_price = data.iloc[n+return_len]\n",
    "        SMA = data.iloc[n-SMA_len:n].mean()\n",
    "        acct_value.append(timing)\n",
    "        if (price >= SMA) & (hold =='stock'): #the first case for when the loop checks for a price signal\n",
    "            timing = next_price/price*timing\n",
    "        elif (price >= SMA) & (hold =='cash'):#the second case for when the loop checks for a price signal\n",
    "            hold = 'stock'\n",
    "            open_position = price\n",
    "            timing = next_price/price*timing\n",
    "        elif (price < SMA) & (hold =='stock'):\n",
    "            #the third and final case for when the loop checks for a price signal else nothing happens.\n",
    "            hold ='cash'\n",
    "            close_position = price\n",
    "            if close_position > open_position:\n",
    "                trades.append(1)\n",
    "            else: \n",
    "                trades.append(0)\n",
    "    return(timing,acct_value,trades)\n",
    "\n",
    "#The SMA function,includes current day included SMA. basic form is to compare price to SMA of length set by user.\n",
    "#This function looks for another price signal after a number of days equal to the return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total daily contract volume\n",
    "\n",
    "def daily_call_volume(ticker):\n",
    "    day_vol = 0\n",
    "    exp_dates = options.get_expiration_dates(ticker)\n",
    "    for dates in exp_dates:\n",
    "        calls = options.get_calls(ticker,dates)[\"Volume\"].values\n",
    "        calls = np.where(calls == '-', 0, calls)\n",
    "        day_vol += calls.astype('int').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old and defunct but still used in some project\n",
    "def conditional_relative_performance(df,select_intervals=[.02,.01,.005,0,-.005,-.01,-.02]):\n",
    "    #select_intervals = [.02,.01,0,-.01,-.02] #highest to lowest\n",
    "    outperformance = np.zeros(len(select_intervals)+1)\n",
    "    probabilities = np.zeros(len(select_intervals)+1)\n",
    "    outperformance[0] = df[df['SPY']>=select_intervals[0]].mean().diff()[1]\n",
    "    probabilities[0] = len(df[df['SPY']>=select_intervals[0]])/len(df)\n",
    "\n",
    "    for i in range(1,len(select_intervals)):\n",
    "        outperformance[i] = df[(df['SPY']<select_intervals[i-1]) & (df['SPY']>select_intervals[i])].mean().diff()[1]\n",
    "        probabilities[i] = len(df[(df['SPY']<select_intervals[i-1]) & (df['SPY']>select_intervals[i])])/len(df)\n",
    "    \n",
    "    outperformance[-1] = df[df['SPY']<=select_intervals[-1]].mean().diff()[1]\n",
    "    probabilities[-1] = len(df[df['SPY']<=select_intervals[-1]])/len(df)\n",
    "\n",
    "\n",
    "    print('Given SPY moves more than',select_intervals[0], 'SPYC outperformance is', np.round(outperformance[0],4), '    Probability:', np.round(probabilities[0],4))\n",
    "    for i in range(len(outperformance)-2):\n",
    "        print('{0: <16}'.format('Given SPY moves'), round(select_intervals[i+1],4),'to',round(select_intervals[i],4),'SPYC outperformance is',np.round(outperformance[i+1],4),'Probability:', np.round(probabilities[i+1],3) )\n",
    "    print('Given SPY moves below ',select_intervals[-1], 'SPYC outperformance is', np.round(outperformance[-1],4),'    Probability:', np.round(probabilities[-1],4))  \n",
    "    print('Expected outperformance: ', np.dot(probabilities,outperformance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that returns allocation changes\n",
    "def hrp_signal(weights, row, data, rebalance_period, formation_period):\n",
    "    formation = data[row - formation_period : row - 1]\n",
    "    cov,corr = formation.cov(),formation.corr()     #2) compute correl matrix \n",
    "    dist=correlDist(corr)                #3) cluster\n",
    "    link=sch.linkage(dist,'single')\n",
    "    sortIx=getQuasiDiag(link)\n",
    "    sortIx=corr.index[sortIx].tolist()   # recover labels\n",
    "    df0=corr.loc[sortIx,sortIx]          # reorder\n",
    "    hrp=getRecBipart(cov,sortIx)         #allocation\n",
    "    hrp.index=data.columns\n",
    "    return np.array(hrp)\n",
    "\n",
    "#signal col names should be a list\n",
    "def backtest_w_signals(data, signal_col_names,rebalance_period, formation_period, hrp_converge_rebals, ivp_converge_rebal,RebalanceOffset=0): \n",
    "    stocks = data.iloc[:,~data.columns.isin(signal_col_names)]\n",
    "    signals = data.iloc[:,data.columns.isin(signal_col_names)]\n",
    "    wealth = np.ones(int(len(stocks.T.index)))/len(stocks.T.index)\n",
    "    wealth_arr = [1]\n",
    "    consecutive_turbulant_days = 0\n",
    "    \n",
    "    for n in range (formation_period, len(stocks)): #shorten for testing\n",
    "        for i in range(len(wealth)):\n",
    "            wealth[i]=wealth[i]*(1+stocks.iloc[n,i])\n",
    "        total_wealth = sum(wealth)\n",
    "        weights = np.array(wealth)/total_wealth\n",
    "        \n",
    "            \n",
    "        if signals['Turbulance'][n]>(np.exp(-.1*(n-100))+2.2)*signals['Turbulance'][:n].std(): #exponential decay was created from plotting the function\n",
    "            consecutive_turbulant_days+=1\n",
    "            if consecutive_turbulant_days>=2:    \n",
    "                formation = stocks[n - formation_period : n - 1]\n",
    "                cov = formation.cov()\n",
    "                ivp_weights = getIVP(cov)\n",
    "                inverse_ivp_rebal = -(ivp_weights - weights)\n",
    "                inv_ivp_scaled = inverse_ivp_rebal/ivp_converge_rebal\n",
    "                \n",
    "                #if rebalance would push an asset negative then split the excess subtraction between the other assets\n",
    "                test = weights + inv_ivp_scaled\n",
    "                scaling_array = np.zeros([len(weights)]) #sets hard boundary to keep this a long only portfolio\n",
    "                for asset in range(len(test)):\n",
    "                    if test[asset] >=0:\n",
    "                        pass\n",
    "                    while test[asset]<0:\n",
    "                        test[asset] = test[asset]+.001\n",
    "                        scaling_array[asset] = scaling_array[asset]+.001\n",
    "                        if test[asset]>=0:\n",
    "                            break\n",
    "                amount_of_scaling = sum(scaling_array)/len(np.argwhere(scaling_array==0))\n",
    "                \n",
    "                for locations in np.argwhere(scaling_array==0):\n",
    "                    test[locations]=test[locations]-amount_of_scaling #flipping this negative sign adds leverage which works great!\n",
    "                inverse_ivp_rebal = test - weights\n",
    "        else:\n",
    "            inverse_ivp_rebal = weights * 0\n",
    "            consecutive_turbulant_days = 0\n",
    "            \n",
    "\n",
    "        if ((n+RebalanceOffset) % rebalance_period == 0):   #rebalance\n",
    "            hrp_weights = hrp_signal(weights, n, stocks, rebalance_period, formation_period)\n",
    "            hrp_rebal = (hrp_weights - weights) / hrp_converge_rebals\n",
    "        else:\n",
    "            hrp_rebal = weights * 0\n",
    "            \n",
    "        #then rebalance by adding signals together\n",
    "        weights_post_rebal = weights + inverse_ivp_rebal + hrp_rebal\n",
    "        \n",
    "        for asset in range(len(wealth)):\n",
    "            wealth[asset] = total_wealth * weights_post_rebal[asset]\n",
    "\n",
    "        wealth_arr.append(sum(wealth))\n",
    "    wealth_arr.append(sum(wealth))\n",
    "    \n",
    "    return(wealth_arr)\n",
    "\n",
    "#wrapper to call backtest with signals function and control for RebalanceTimingLuck\n",
    "def backtest_w_RBL(*args):\n",
    "    number_of_iterations = np.floor(np.array(args[2])/5)\n",
    "    array_pre_average = np.zeros([len(args[0])-args[3]+2,int(number_of_iterations)]) #plus two comes from my formulation of backtesting function\n",
    "    for n in range(int(number_of_iterations)):\n",
    "        array_pre_average[:,n] = backtest_w_signals(args[0],args[1],args[2],args[3],args[4],args[5],RebalanceOffset=n*5)\n",
    "        #take average of backtests \n",
    "    return(array_pre_average.mean(axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "def plot_learning_curve(\n",
    "    estimator,\n",
    "    X,\n",
    "    y,\n",
    "):\n",
    "\n",
    "  train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator,\n",
    "        X = X,\n",
    "        y = y,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=5,\n",
    "    )\n",
    "  train_scores_mean = -np.mean(train_scores, axis=1)\n",
    "\n",
    "  train_scores_std = np.std(train_scores, axis=1)\n",
    "\n",
    "  test_scores_mean = -np.mean(test_scores, axis=1)\n",
    "\n",
    "  test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "\n",
    "  _, axes = plt.subplots(1, 1, figsize=(20, 5))\n",
    "  axes.set_xlabel(\"Training examples\")\n",
    "  axes.set_ylabel(\"Score\")\n",
    "\n",
    "  axes.grid()\n",
    "  axes.fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "  axes.fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "  axes.plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "  axes.plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "  axes.legend(loc=\"best\")\n",
    "  print(test_scores_mean[-1])\n",
    "  print(train_scores_mean[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give pandas series for x and y\n",
    "def time_series_processing(x,y,sequence_len):\n",
    "    ts_processing = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x[:-sequence_len],\n",
    "    y[sequence_len:],\n",
    "    sequence_length=sequence_len, \n",
    "    sampling_rate=1,\n",
    "    batch_size=len(x),\n",
    "    sequence_stride=1)\n",
    "    \n",
    "    for batch in ts_processing.take(1):\n",
    "        batched_inputs, targets = batch\n",
    "    return batched_inputs.numpy(), targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
